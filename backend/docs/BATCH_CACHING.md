# Batch Prompt Caching System

## Overview

The HealthAssist backend now uses a **batch prompt caching system** that provides zero-latency responses by pre-populating the cache with actual `/analyze` endpoint responses for canonical questions. This approach is similar to OpenAI's prompt caching and ensures consistent, production-grade responses.

## How It Works

1. **Canonical Questions**: A set of pre-defined questions are stored in `docs/questions_preprocessed`
2. **Batch Population**: The `/api/cache/batch-refresh` endpoint calls the `/analyze` endpoint for each canonical question
3. **Real Responses**: The actual responses from `/analyze` are cached (not hand-written answers)
4. **Semantic Matching**: When a user asks a question, the system finds similar cached questions using embeddings
5. **Zero Latency**: If a match is found above the similarity threshold (70%), the cached response is returned instantly

## API Endpoints

### Batch Cache Management

#### `POST /api/cache/batch-refresh`
Triggers batch cache population by calling `/analyze` for all canonical questions.

**Response:**
```json
{
  "status": "success",
  "message": "Batch cache refresh completed",
  "summary": {
    "total_questions": 15,
    "successful": 15,
    "failed": 0,
    "processing_time": "45.2 seconds"
  },
  "cache_stats": {
    "total_cached_questions": 15,
    "cache_file_size": 125000,
    "last_updated": "2025-06-21T21:57:48.218588"
  }
}
```

#### `GET /api/cache/canonical-questions`
Lists all canonical questions used for batch caching.

**Response:**
```json
{
  "status": "success",
  "total_questions": 15,
  "questions": [
    "What security measures are in place to ensure HIPAA compliance?",
    "How does HealthAssist integrate with EMR systems?",
    "..."
  ]
}
```

#### `GET /api/cache/stats`
Returns cache statistics.

#### `POST /api/cache/clear`
Clears all cached answers.

### Primary Endpoint

#### `POST /analyze`
The main conversation analysis endpoint that uses batch caching.

**Cache Behavior:**
- First checks for cached answers using semantic similarity (70% threshold)
- If cache hit: Returns cached response with zero latency
- If cache miss: Processes with full RAG pipeline and optionally caches the result

## Setup Instructions

1. **Populate Canonical Questions**: Add questions to `docs/questions_preprocessed` (one per line)

2. **Initial Cache Population**:
   ```bash
   curl -X POST http://localhost:8000/api/cache/batch-refresh
   ```

3. **Verify Cache**:
   ```bash
   curl -X GET http://localhost:8000/api/cache/stats
   ```

## Benefits

- **Zero Latency**: Cached responses return instantly (sub-ms response times)
- **Consistent Quality**: Responses are generated by the same `/analyze` pipeline
- **Production Ready**: Uses actual endpoint responses, not hand-written answers
- **Semantic Matching**: Uses embeddings for intelligent question similarity
- **Easy Management**: Simple API endpoints for cache operations

## Configuration

- **Similarity Threshold**: 70% (configurable in `/analyze` endpoint)
- **Cache Location**: `backend/app/uploads/cache_documents/answer_cache.json`
- **Canonical Questions**: `backend/docs/questions_preprocessed`

## Monitoring

Use the `/api/cache/stats` endpoint to monitor:
- Total cached questions
- Cache file size
- Last update timestamp

## Maintenance

- **Regular Updates**: Run `/api/cache/batch-refresh` when canonical questions change
- **Cache Clearing**: Use `/api/cache/clear` to reset the cache
- **Question Management**: Update `docs/questions_preprocessed` as needed

## Technical Details

- **Embeddings**: Uses OpenAI's `text-embedding-3-small` model for semantic similarity
- **Storage**: JSON file with embeddings and response data
- **Similarity**: Cosine similarity with numpy for fast matching
- **Background Processing**: Batch population runs asynchronously with progress tracking
